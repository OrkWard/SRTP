# 2022.8#4
本周我们做的工作集中在参数的调整和另一些模型的实现上。

首先我们实现了 [Contextual deep CNN based hyperspectral classification, Hyungtae Lee and Heesung Kwon, IGARSS 2016](https://ieeexplore.ieee.org/document/7729859) 中描述的模型，该模型的特点是在第一步进行了一次 3x3xbands 卷积核和 1x1xbands 卷积核的 3D 卷积并混合，在一次卷积中提取的单个像元的所有光谱信息，后续只进行 2D 卷积。

首先对月初的模型（取人名为简称）在各种参数下进行测试。

Hamida（50 个周期，使用 PaviaU 数据集，随机抽样）：

| Train percent | Patch size | Learning rate | Kappa | Time |
| :-----------: | :--------: | :-----------: | :---: | :--: |
| 50%           | 3          | 0.001         | 0.956 | 5:00 |
| 50%           | 5          | 0.001         | 0.953 | 7:57 |
| 50%           | 7          | 0.001         | 0.944 | 23:42|
| 50%           | 5          | 0.003         | 0.949 | 7:33 |
| 50%           | 5          | 0.005         | 0.951 | 7:45 |
| 50%           | 5          | 0.01          | 0.923 | 7:27 |
| 50%           | 5          | 0.0005        | 0.945 | 7:14 |
| 30%           | 5          | 0.01          | 0.92  | 3:46 |
| 10%           | 5          | 0.01          | 0.849 | 0:31 |

由于训练环境并不稳定（其他占用 GPU 的任务），相同参数的消耗时间会有微小的差距，但数量级的差距是可以体现的。

在上面的的测试中，时间明显地受到训练集大小和 Patch 大小的影响，这是很容易理解的。该网络没有反卷积过程，而是对每个输入像素连同它的周边进行卷积，得到的结果作为中心像元的分类结果，因此 Patch 增加意味着参数量的增加从而影响了消耗时间。

比较反直觉的一点是 Patch 增加同时导致了时间的增加和准确度的下降。我们认为一种可能的原因是该数据集分辨率低，增加的像素虽然引入了信息量，但由于实际距离较远，相关度不高反而成了噪音。由于至少需要 3x3 的输入大小进行卷积，无法对更小的输入进行测试。

learning rate 虽然在一部分模型中影响较大，但在我们的测试中只影响了损失的收敛速度（由于随机的初始化该比较也并不准确），对结果的准确度和训练时间影响不大，除非过大导致难以收敛。

下面是另一个模型的测试结果：

Lee（50 个周期，PaviaU 数据集，随机抽样）：


| Train percent | Patch size | Learning rate | Kappa | Time |
| :-----------: | :--------: | :-----------: | :---: | :--: |
| 50%           | 5          | 0.001         | 0.934 |11:04 |
| 50%           | 3          | 0.001         | 0.969 | 6:04 |
| 50%           | 7          | 0.001         | 0.974 |15:58 |
| 50%           | 9          | 0.001         | 0.978 |23:16 |
| 30%           | 5          | 0.001         | 0.945 | 6:27 |
| 30%           | 5          | 0.003         | 0.942 | 6:28 |
| 30%           | 5          | 0.005         | 0.930 | 6:30 |

这个模型明显地受到了 Patch 大小的增益。同样也不容易受到学习率的影响，减小训练集也一如预期地降低了时间和准确度。两者之间的差异还有待进一步对细节的比较。

下周我们也会主要集中在参数的调整、对比和原因分析上。在模型构建的过程中我们注意到一些特别的数据预处理方式（旋转、反转图像、类别均衡等），这次我们都略去了，以及网络中陌生的层（Dropout、LRN等）。接下来我们会更细致地研究这些模型，并尝试做出一些改动，或者理解为什么这么做是有效的。