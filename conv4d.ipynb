{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myConv1d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((out_channels, in_channels, kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        self.bias = torch.randn((out_channels, ), requires_grad=True)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size, 'input to less!'\n",
    "        calculated = torch.zeros((self.out_channels, input.size(1) - self.kernel_size + 1))\n",
    "        for i_out in range(self.out_channels):\n",
    "            for i_in in range(self.in_channels):\n",
    "                for i_w in range(calculated.shape[1]):\n",
    "                    # print(i_out, i_in, i_w)\n",
    "                    calculated[i_out][i_w] += torch.dot(self.parameters[i_out, i_in], input[i_in, i_w:i_w + self.kernel_size])\n",
    "        calculated += self.bias.reshape((self.out_channels, 1))\n",
    "        return calculated\n",
    "\n",
    "IN_CHANNEL = 1\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = 3\n",
    "HEIGHT = 3\n",
    "WIDTH = 3\n",
    "LENGTH = 4\n",
    "t1 = torch.arange(IN_CHANNEL * LENGTH, dtype=torch.float)\n",
    "t1.resize_(IN_CHANNEL, LENGTH)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1d = nn.Conv1d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE, dtype=torch.float)\n",
    "l1d_my = myConv1d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "# d2.parameters = d1._parameters['weight'].data\n",
    "# d2.bias = d1._parameters['bias'].data\n",
    "# d2(t), d1(t)\n",
    "l1d_my(t1).sum().backward()\n",
    "l1d_my.parameters.grad, l1d_my.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myConv2d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple[int, int], stride: int = 1, bias: bool = True) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((self.out_channels, self.in_channels, *kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        if bias:\n",
    "            self.bias = torch.randn((self.out_channels, ), requires_grad=True)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size[0] and input.shape[2] >= self.kernel_size[1], 'input to less!'\n",
    "        calculated = torch.zeros(self.out_channels, input.shape[1] - self.kernel_size[0] + 1, input.shape[2] - self.kernel_size[1] + 1)\n",
    "        for i_out in range(calculated.shape[0]):\n",
    "            for i_in in range(self.in_channels):\n",
    "                for i_w in range(calculated.shape[1]):\n",
    "                    for i_h in range(calculated.shape[2]):\n",
    "                        calculated[i_out][i_w][i_h] += torch.sum(self.parameters[i_out, i_in] * input[i_in, i_w:i_w + self.kernel_size[0], i_h:i_h + self.kernel_size[1]])\n",
    "        if self.use_bias:\n",
    "            calculated += self.bias.reshape((self.out_channels, 1, 1))\n",
    "        return calculated\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "        if self.use_bias:\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "# layer init parameters\n",
    "IN_CHANNEL = 2\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = (3, 3)\n",
    "# input tensor: in_channels * width * height\n",
    "HEIGHT = 4\n",
    "WIDTH = 4\n",
    "t2 = torch.arange(IN_CHANNEL * WIDTH * HEIGHT, dtype=torch.float)\n",
    "t2.resize_(IN_CHANNEL, WIDTH, HEIGHT)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2d = nn.Conv2d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "l2d_my = myConv2d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "# d4.parameters = d3._parameters['weight'].data\n",
    "# d4.bias = d3._parameters['bias'].data\n",
    "# d3(t1), d4(t1)\n",
    "l2d_my(t2).sum().backward()\n",
    "l2d_my.parameters.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d convolution operate\n",
    "def corr2d(X: torch.Tensor, K: torch.Tensor):  \n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "K = torch.tensor([[1.0, -1.0]]) # <---target\n",
    "Y = corr2d(X, K)\n",
    "# conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "conv2d = myConv2d(1, 1, kernel_size=K.shape, bias=False)\n",
    "\n",
    "X = X.reshape((1, 6, 8))\n",
    "Y = Y.reshape((1, 6, 7))\n",
    "lr = 3e-2  # learn rate\n",
    "\n",
    "for i in range(20):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    l.sum().backward()\n",
    "    with torch.no_grad():\n",
    "        conv2d.parameters -= lr * conv2d.parameters.grad\n",
    "    conv2d.zero_grad()\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')\n",
    "conv2d.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myConv3d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple[int, int, int], stride: int = 1, bias: bool = True) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((self.out_channels, self.in_channels, *kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        if bias:\n",
    "            self.bias = torch.randn((self.out_channels, ), requires_grad=True)\n",
    "    \n",
    "    # calculate conv3d use kernel Y on X\n",
    "    @staticmethod\n",
    "    def conv3d(X: torch.Tensor, Y: torch.Tensor, output: torch.Tensor):\n",
    "        depth_cal = X.shape[0] - Y.shape[0] + 1\n",
    "        width_cal = X.shape[1] - Y.shape[1] + 1\n",
    "        height_cal = X.shape[2] - Y.shape[2] + 1\n",
    "        for i_d in range(depth_cal):\n",
    "            for i_w in range(width_cal):\n",
    "                for i_h in range(height_cal):\n",
    "                    output[i_d][i_w][i_h] += torch.sum(X[i_d:i_d + Y.shape[0], \\\n",
    "                                                         i_w:i_w + Y.shape[1], \\\n",
    "                                                         i_h:i_h + Y.shape[2]] * Y)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size[0] and input.shape[2] >= self.kernel_size[1] and input.shape[3] >= self.kernel_size[2], 'input to less!'\n",
    "        calculated = torch.zeros(self.out_channels, input.shape[1] - self.kernel_size[0] + 1, \\\n",
    "                                                    input.shape[2] - self.kernel_size[1] + 1, \\\n",
    "                                                    input.shape[3] - self.kernel_size[2] + 1)\n",
    "        for i_out in range(self.out_channels):\n",
    "            for i_in in range(self.in_channels):\n",
    "                self.conv3d(input[i_in], self.parameters[i_out, i_in], calculated[i_out])\n",
    "        if self.use_bias:\n",
    "            calculated += self.bias.reshape((self.out_channels, 1, 1, 1))\n",
    "        return calculated\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "        if self.use_bias:\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "# layer init parameters\n",
    "IN_CHANNEL = 3\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = (2, 2, 1)\n",
    "# input tensor: in_channels * depth * width * height\n",
    "DEPTH = 4\n",
    "WIDTH = 3\n",
    "HEIGHT = 2\n",
    "t3 = torch.arange(IN_CHANNEL * DEPTH * WIDTH * HEIGHT, dtype=torch.float)\n",
    "t3.resize_(IN_CHANNEL, DEPTH, WIDTH, HEIGHT)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3d = nn.Conv3d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "l3d_my = myConv3d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "\n",
    "# compare\n",
    "l3d_my.parameters = l3d._parameters['weight'].data\n",
    "l3d_my.bias = l3d._parameters['bias'].data\n",
    "l3d(t3).sum(), l3d_my(t3).sum()\n",
    "\n",
    "# backward\n",
    "# l3d_my(t3).sum().backward()\n",
    "# l3d_my.parameters.grad, l3d_my.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d convolution operate\n",
    "def corr3d(X: torch.Tensor, K: torch.Tensor):  \n",
    "    d, w, h = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - d + 1, X.shape[1] - w + 1, X.shape[2] - h + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            for k in range(Y.shape[2]):\n",
    "                Y[i, j, k] = (X[i:i + d, j:j + w, k:k + h] * K).sum()\n",
    "    return Y\n",
    "\n",
    "X = torch.ones((6, 8, 7))\n",
    "X[:, 2:6, 3:5] = 0\n",
    "K = torch.tensor([[[1.0, -1.0], [-1.0, 1.0]]]) # <---target\n",
    "Y = corr3d(X, K)\n",
    "# conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "conv3d = myConv3d(1, 1, kernel_size=K.shape, bias=False)\n",
    "\n",
    "X = X.reshape((1, 6, 8, 7))\n",
    "Y = Y.reshape((1, 6, 7, 6))\n",
    "lr = 7e-4  # learn rate\n",
    "\n",
    "for i in range(100):\n",
    "    Y_hat = conv3d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    l.sum().backward()\n",
    "    with torch.no_grad():\n",
    "        conv3d.parameters -= lr * conv3d.parameters.grad\n",
    "    conv3d.zero_grad()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')\n",
    "conv3d.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myConv4d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple[int, int, int, int], stride: int = 1, bias: bool = True) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((self.out_channels, self.in_channels, *kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        if bias:\n",
    "            self.bias = torch.randn((self.out_channels, ), requires_grad=True)\n",
    "    \n",
    "    # calculate conv3d use kernel Y on X\n",
    "    @staticmethod\n",
    "    def conv4d(X: torch.Tensor, Y: torch.Tensor, output: torch.Tensor):\n",
    "        dim1_cal = X.shape[0] - Y.shape[0] + 1\n",
    "        dim2_cal = X.shape[1] - Y.shape[1] + 1\n",
    "        dim3_cal = X.shape[2] - Y.shape[2] + 1\n",
    "        dim4_cal = X.shape[3] - Y.shape[3] + 1\n",
    "        for i_1 in range(dim1_cal):\n",
    "            for i_2 in range(dim2_cal):\n",
    "                for i_3 in range(dim3_cal):\n",
    "                    for i_4 in range(dim4_cal):\n",
    "                        output[i_1][i_2][i_3][i_4] += torch.sum(X[i_1:i_1 + Y.shape[0], \\\n",
    "                                                            i_2:i_2 + Y.shape[1], \\\n",
    "                                                            i_3:i_3 + Y.shape[2], \\\n",
    "                                                            i_4:i_4 + Y.shape[3]] * Y)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size[0] and \\\n",
    "               input.shape[2] >= self.kernel_size[1] and \\\n",
    "               input.shape[3] >= self.kernel_size[2] and \\\n",
    "               input.shape[4] >= self.kernel_size[3], 'input to less!'\n",
    "        calculated = torch.zeros(self.out_channels, input.shape[1] - self.kernel_size[0] + 1, \\\n",
    "                                                    input.shape[2] - self.kernel_size[1] + 1, \\\n",
    "                                                    input.shape[3] - self.kernel_size[2] + 1, \\\n",
    "                                                    input.shape[4] - self.kernel_size[3] + 1)\n",
    "        for i_out in range(self.out_channels):\n",
    "            for i_in in range(self.in_channels):\n",
    "                self.conv4d(input[i_in], self.parameters[i_out, i_in], calculated[i_out])\n",
    "        if self.use_bias:\n",
    "            calculated += self.bias.reshape((self.out_channels, 1, 1, 1, 1))\n",
    "        return calculated\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "        if self.use_bias:\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "# layer init parameters\n",
    "IN_CHANNEL = 2\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = (2, 2, 1, 3)\n",
    "# input tensor: in_channels * depth * width * height\n",
    "DIM1 = 4\n",
    "DIM2 = 3\n",
    "DIM3 = 2\n",
    "DIM4 = 4\n",
    "t4 = torch.arange(IN_CHANNEL * DIM1 * DIM2 * DIM3 * DIM4, dtype=torch.float)\n",
    "t4.resize_(IN_CHANNEL, DIM1, DIM2, DIM3, DIM4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21986.5996, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4d_my = myConv4d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "\n",
    "# backward\n",
    "l4d_my(t4).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
