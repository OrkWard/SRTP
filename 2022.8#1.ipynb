{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022.8#1\n",
    "本周我们复现的模型为[3-D Deep Learning Approach for Remote Sensing Image Classification Amina Ben Hamida, Alexandre Benoit, Patrick Lambert, Chokri Ben Amar IEEE TGRS, 2018](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344565)中介绍的模型，一个基于 3D CNN 的高光谱影像分类。\n",
    "\n",
    "我们参考的实现使用 visdom 进行可视化，tqdm 展示进度条。我们在 Jupyter 内重新实现本来应该使用 matplotlib 进行可视化， 本次实现略过了可视化部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "第一部分，设置各种超参数并从文件中读取数据。这部分的变量都是可以调整的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "\n",
    "# 参数集\n",
    "params = {\n",
    "    'dataset': 'Pavia',\n",
    "    'training_sample': 0.1, # 用于训练的数据，其余用于测试\n",
    "    'epoch': 2,\n",
    "    'batch_size': 100,\n",
    "    'lr': 0.001,\n",
    "}\n",
    "\n",
    "# 使用 GPU\n",
    "CUDA_DEVICE = torch.device('cuda:0')\n",
    "\n",
    "# 加载数据集，这里使用Pavia\n",
    "path = './dataset/Pavia/'\n",
    "img = io.loadmat(path + 'Pavia.mat')['pavia']\n",
    "gt = io.loadmat(path + 'Pavia_gt.mat')['pavia_gt']\n",
    "label_values = [\n",
    "    \"Undefined\",\n",
    "    \"Water\",\n",
    "    \"Trees\",\n",
    "    \"Asphalt\",\n",
    "    \"Self-Blocking Bricks\",\n",
    "    \"Bitumen\",\n",
    "    \"Tiles\",\n",
    "    \"Shadows\",\n",
    "    \"Meadows\",\n",
    "    \"Bare Soil\",\n",
    "]\n",
    "# 归一化\n",
    "img = np.asarray(img, dtype=\"float32\")\n",
    "img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "\n",
    "num_classes = len(label_values)\n",
    "num_bands = img.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分离训练集和测试集\n",
    "传感器提供的数据是一个整体，需要手动将数据（ground truth部分）分为训练集和测试集，用空值填充其余部分（相当于 Undefined 类）。scikit-learn 包提供了一个将列表按指定的比例或数量分离的工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在 148152 个样本中选择了 14815 个'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "# 后续需要利用该函数进行交叉验证\n",
    "def sample_gt(gt, train_size):\n",
    "    indices = np.nonzero(gt)\n",
    "    X = list(zip(*indices)) # x,y features\n",
    "    y = gt[indices].ravel() # classes\n",
    "    train_gt = np.zeros_like(gt)\n",
    "    test_gt = np.zeros_like(gt)\n",
    "    if train_size > 1:\n",
    "       train_size = int(train_size)\n",
    "\n",
    "    train_indices, test_indices = sklearn.model_selection.train_test_split(X, train_size=train_size, stratify=y)\n",
    "    train_indices = [list(t) for t in zip(*train_indices)]\n",
    "    test_indices = [list(t) for t in zip(*test_indices)]\n",
    "    train_gt[tuple(train_indices)] = gt[tuple(train_indices)]\n",
    "    test_gt[tuple(test_indices)] = gt[tuple(test_indices)]\n",
    "    return train_gt, test_gt\n",
    "\n",
    "train_gt, test_gt = sample_gt(gt, params['training_sample'])\n",
    "\"在 {} 个样本中选择了 {} 个\" .format(np.count_nonzero(gt), np.count_nonzero(train_gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Module(nn.Module):\n",
    "    # 初始化模型参数\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv3d)):\n",
    "            init.kaiming_normal_(m.weight)\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "    def __init__(self, input_channels, n_classes, patch_size=5, dilation=1):\n",
    "        super(Module, self).__init__()\n",
    "        # 第一层卷积层，（3，3，3）卷积核，步长为1，20个节点\n",
    "        self.patch_size = patch_size\n",
    "        self.input_channels = input_channels\n",
    "        dilation = (dilation, 1, 1)\n",
    "\n",
    "        if patch_size == 3:\n",
    "            self.conv1 = nn.Conv3d(1, 20, (3, 3, 3), stride=(1, 1, 1), dilation=dilation, padding=1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv3d(1, 20, (3, 3, 3), stride=(1, 1, 1), dilation=dilation, padding=0)\n",
    "\n",
    "        # 第二层池化层，（1，1，3）一维卷积核，步长为2以在光谱方向上减少维度\n",
    "        self.pool1 = nn.Conv3d(20, 20, (3, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))\n",
    "\n",
    "        # 然后是前两层的重复，节点数量为35\n",
    "        self.conv2 = nn.Conv3d(20, 35, (3, 3, 3), dilation=dilation, stride=(1, 1, 1), padding=(1, 0, 0))\n",
    "        self.pool2 = nn.Conv3d(35, 35, (3, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))\n",
    "\n",
    "        # 进行两次一维卷积\n",
    "        self.conv3 = nn.Conv3d(35, 35, (3, 1, 1), dilation=dilation, stride=(1, 1, 1), padding=(1, 0, 0))\n",
    "        self.conv4 = nn.Conv3d(35, 35, (2, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.features_size = self._get_final_flattened_size()\n",
    "        \n",
    "        # 最后是单独的全连接层\n",
    "        self.fc = nn.Linear(self.features_size, n_classes)\n",
    "\n",
    "        self.apply(self.weight_init)\n",
    "\n",
    "    # 计算可变输入展开后的列表长度\n",
    "    def _get_final_flattened_size(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(\n",
    "                (1, 1, self.input_channels, self.patch_size, self.patch_size)\n",
    "            )\n",
    "            x = self.pool1(self.conv1(x))\n",
    "            x = self.pool2(self.conv2(x))\n",
    "            x = self.conv3(x)\n",
    "            x = self.conv4(x)\n",
    "            _, t, c, w, h = x.size()\n",
    "        return t * c * w * h\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 全部使用ReLU激活函数\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(-1, self.features_size)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Module(num_bands, num_classes)\n",
    "net = net.to(CUDA_DEVICE)\n",
    "# 随机梯度下降\n",
    "optimizer = optim.SGD(net.parameters(), params['lr'], weight_decay=0.0005)\n",
    "# 交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.ones(num_classes).to(CUDA_DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载器\n",
    "此时的数据仍为以 narray 形式存储的数集，需要实现一个继承类以用 Pytorch 提供的 api 进行 batch 级的数据加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "train_gt, val_gt = sample_gt(train_gt, 0.95)\n",
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, data, gt, **params):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.label = gt\n",
    "        self.name = params[\"dataset\"]\n",
    "        self.patch_size = 5\n",
    "        mask = np.ones_like(gt)\n",
    "        x_pos, y_pos = np.nonzero(mask)\n",
    "        p = self.patch_size // 2\n",
    "        self.indices = np.array(\n",
    "            [\n",
    "                (x, y)\n",
    "                for x, y in zip(x_pos, y_pos)\n",
    "                if x > p and x < data.shape[0] - p and y > p and y < data.shape[1] - p\n",
    "            ]\n",
    "        )\n",
    "        self.labels = [self.label[x, y] for x, y in self.indices]\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.indices[i]\n",
    "        x1, y1 = x - self.patch_size // 2, y - self.patch_size // 2\n",
    "        x2, y2 = x1 + self.patch_size, y1 + self.patch_size\n",
    "\n",
    "        data = self.data[x1:x2, y1:y2]\n",
    "        label = self.label[x1:x2, y1:y2]\n",
    "\n",
    "        # Copy the data into numpy arrays (PyTorch doesn't like numpy views)\n",
    "        data = np.asarray(np.copy(data).transpose((2, 0, 1)), dtype=\"float32\")\n",
    "        label = np.asarray(np.copy(label), dtype=\"int64\")\n",
    "\n",
    "        # 将数据加载为 Tensor\n",
    "        data = torch.from_numpy(data)\n",
    "        label = torch.from_numpy(label)\n",
    "        label = label[self.patch_size // 2, self.patch_size // 2]\n",
    "\n",
    "        # Add a fourth dimension for 3D CNN\n",
    "        if self.patch_size > 1:\n",
    "            # Make 4D data ((Batch x) Planes x Channels x Width x Height)\n",
    "            data = data.unsqueeze(0)\n",
    "        return data, label\n",
    "\n",
    "train_dataset = Dataset(img, train_gt, **params)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "val_dataset = Dataset(img, val_gt, **params)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01686476171016693\n",
      "0.07508590072393417\n",
      "0.03868788480758667\n",
      "0.2064131498336792\n",
      "0.22261665761470795\n",
      "0.04249861091375351\n",
      "0.1363358497619629\n",
      "0.10841484367847443\n",
      "0.052877526730298996\n",
      "0.08296363800764084\n",
      "0.1536632478237152\n",
      "0.019730927422642708\n",
      "0.07960836589336395\n",
      "0.1741037517786026\n",
      "0.14908188581466675\n",
      "0.05638481676578522\n",
      "0.016952678561210632\n",
      "0.06932863593101501\n",
      "0.016139334067702293\n",
      "0.012532655149698257\n",
      "0.08281221240758896\n",
      "0.02270493470132351\n",
      "0.05635599046945572\n",
      "0.1597495973110199\n",
      "0.04146759957075119\n",
      "0.07195183634757996\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, params['epoch'] + 1):\n",
    "    net.train()\n",
    "    iter_ = 0\n",
    "    base2 = 1\n",
    "\n",
    "    for train_data, target in train_loader:\n",
    "        train_data, target = train_data.to(CUDA_DEVICE), target.to(CUDA_DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(train_data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 输出损失\n",
    "        if iter_ == base2:\n",
    "            base2 *= 2\n",
    "            print(loss.item())\n",
    "        iter_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果测试\n",
    "暂时使用 visdom 可视化的结果截图。\n",
    "\n",
    "![train loss 和 validation accuracy](file:///d%3A/repos/Hyperspectral/static/QQ%E6%88%AA%E5%9B%BE20220808152636.jpg)\n",
    "\n",
    "![混淆矩阵](file:///d%3A/repos/Hyperspectral/static/QQ%E6%88%AA%E5%9B%BE20220808152651.jpg)\n",
    "\n",
    "![预测结果](file:///d%3A/repos/Hyperspectral/static/QQ%E6%88%AA%E5%9B%BE20220808152720.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a47e9c955df6178b2e3d07b55dfed71eee1bcf21dc3da9caa7365344dfc4fda2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
