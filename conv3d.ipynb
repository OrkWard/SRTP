{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myConv1d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((out_channels, in_channels, kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        self.bias = torch.randn((out_channels, ), requires_grad=True)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size, 'input to less!'\n",
    "        calculated = torch.zeros((self.out_channels, input.size(1) - self.kernel_size + 1))\n",
    "        for i_out in range(self.out_channels):\n",
    "            for i_in in range(self.in_channels):\n",
    "                for i_w in range(calculated.shape[1]):\n",
    "                    # print(i_out, i_in, i_w)\n",
    "                    calculated[i_out][i_w] += torch.dot(self.parameters[i_out, i_in], input[i_in, i_w:i_w + self.kernel_size])\n",
    "        calculated += self.bias.reshape((self.out_channels, 1))\n",
    "        return calculated\n",
    "\n",
    "IN_CHANNEL = 1\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = 3\n",
    "HEIGHT = 3\n",
    "WIDTH = 3\n",
    "LENGTH = 4\n",
    "t1 = torch.arange(IN_CHANNEL * LENGTH, dtype=torch.float)\n",
    "t1.resize_(IN_CHANNEL, LENGTH)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 3., 5.]],\n",
       " \n",
       "         [[1., 3., 5.]]]),\n",
       " tensor([2., 2.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1d = nn.Conv1d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE, dtype=torch.float)\n",
    "l1d_my = myConv1d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "# d2.parameters = d1._parameters['weight'].data\n",
    "# d2.bias = d1._parameters['bias'].data\n",
    "# d2(t), d1(t)\n",
    "l1d_my(t1).sum().backward()\n",
    "l1d_my.parameters.grad, l1d_my.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27.],\n",
       "         [28., 29., 30., 31.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myConv2d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple[int, int], stride: int = 1, bias: bool = True) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((self.out_channels, self.in_channels, *kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        if bias:\n",
    "            self.bias = torch.randn((self.out_channels, ), requires_grad=True)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size[0] and input.shape[2] >= self.kernel_size[1], 'input to less!'\n",
    "        calculated = torch.zeros(self.out_channels, input.shape[1] - self.kernel_size[0] + 1, input.shape[2] - self.kernel_size[1] + 1)\n",
    "        for i_out in range(calculated.shape[0]):\n",
    "            for i_in in range(self.in_channels):\n",
    "                for i_w in range(calculated.shape[1]):\n",
    "                    for i_h in range(calculated.shape[2]):\n",
    "                        calculated[i_out][i_w][i_h] += torch.sum(self.parameters[i_out, i_in] * input[i_in, i_w:i_w + self.kernel_size[0], i_h:i_h + self.kernel_size[1]])\n",
    "        if self.use_bias:\n",
    "            calculated += self.bias.reshape((self.out_channels, 1, 1))\n",
    "        return calculated\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "        if self.use_bias:\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "# layer init parameters\n",
    "IN_CHANNEL = 2\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = (3, 3)\n",
    "# input tensor: in_channels * width * height\n",
    "HEIGHT = 4\n",
    "WIDTH = 4\n",
    "t2 = torch.arange(IN_CHANNEL * WIDTH * HEIGHT, dtype=torch.float)\n",
    "t2.resize_(IN_CHANNEL, WIDTH, HEIGHT)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 10.,  14.,  18.],\n",
       "           [ 26.,  30.,  34.],\n",
       "           [ 42.,  46.,  50.]],\n",
       " \n",
       "          [[ 74.,  78.,  82.],\n",
       "           [ 90.,  94.,  98.],\n",
       "           [106., 110., 114.]]],\n",
       " \n",
       " \n",
       "         [[[ 10.,  14.,  18.],\n",
       "           [ 26.,  30.,  34.],\n",
       "           [ 42.,  46.,  50.]],\n",
       " \n",
       "          [[ 74.,  78.,  82.],\n",
       "           [ 90.,  94.,  98.],\n",
       "           [106., 110., 114.]]]]),\n",
       " tensor([2., 2.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2d = nn.Conv2d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "l2d_my = myConv2d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "# d4.parameters = d3._parameters['weight'].data\n",
    "# d4.bias = d3._parameters['bias'].data\n",
    "# d3(t1), d4(t1)\n",
    "l2d_my(t2).sum().backward()\n",
    "l2d_my.parameters.grad, l1d_my.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 13.612\n",
      "epoch 4, loss 3.221\n",
      "epoch 6, loss 0.924\n",
      "epoch 8, loss 0.312\n",
      "epoch 10, loss 0.117\n",
      "epoch 12, loss 0.046\n",
      "epoch 14, loss 0.019\n",
      "epoch 16, loss 0.008\n",
      "epoch 18, loss 0.003\n",
      "epoch 20, loss 0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0035, -0.9961]]]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2d convolution operate\n",
    "def corr2d(X: torch.Tensor, K: torch.Tensor):  \n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "K = torch.tensor([[1.0, -1.0]]) # <---target\n",
    "Y = corr2d(X, K)\n",
    "# conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "conv2d = myConv2d(1, 1, kernel_size=K.shape, bias=False)\n",
    "\n",
    "X = X.reshape((1, 6, 8))\n",
    "Y = Y.reshape((1, 6, 7))\n",
    "lr = 3e-2  # learn rate\n",
    "\n",
    "for i in range(20):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    l.sum().backward()\n",
    "    with torch.no_grad():\n",
    "        conv2d.parameters -= lr * conv2d.parameters.grad\n",
    "    conv2d.zero_grad()\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')\n",
    "conv2d.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.],\n",
       "          [ 2.,  3.],\n",
       "          [ 4.,  5.]],\n",
       "\n",
       "         [[ 6.,  7.],\n",
       "          [ 8.,  9.],\n",
       "          [10., 11.]],\n",
       "\n",
       "         [[12., 13.],\n",
       "          [14., 15.],\n",
       "          [16., 17.]],\n",
       "\n",
       "         [[18., 19.],\n",
       "          [20., 21.],\n",
       "          [22., 23.]]],\n",
       "\n",
       "\n",
       "        [[[24., 25.],\n",
       "          [26., 27.],\n",
       "          [28., 29.]],\n",
       "\n",
       "         [[30., 31.],\n",
       "          [32., 33.],\n",
       "          [34., 35.]],\n",
       "\n",
       "         [[36., 37.],\n",
       "          [38., 39.],\n",
       "          [40., 41.]],\n",
       "\n",
       "         [[42., 43.],\n",
       "          [44., 45.],\n",
       "          [46., 47.]]],\n",
       "\n",
       "\n",
       "        [[[48., 49.],\n",
       "          [50., 51.],\n",
       "          [52., 53.]],\n",
       "\n",
       "         [[54., 55.],\n",
       "          [56., 57.],\n",
       "          [58., 59.]],\n",
       "\n",
       "         [[60., 61.],\n",
       "          [62., 63.],\n",
       "          [64., 65.]],\n",
       "\n",
       "         [[66., 67.],\n",
       "          [68., 69.],\n",
       "          [70., 71.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myConv3d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple[int, int, int], stride: int = 1, bias: bool = True) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((self.out_channels, self.in_channels, *kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        if bias:\n",
    "            self.bias = torch.randn((self.out_channels, ), requires_grad=True)\n",
    "    \n",
    "    # calculate conv3d use kernel Y on X\n",
    "    @staticmethod\n",
    "    def conv3d(X: torch.Tensor, Y: torch.Tensor, output: torch.Tensor):\n",
    "        depth_cal = X.shape[0] - Y.shape[0] + 1\n",
    "        width_cal = X.shape[1] - Y.shape[1] + 1\n",
    "        height_cal = X.shape[2] - Y.shape[2] + 1\n",
    "        for i_d in range(depth_cal):\n",
    "            for i_w in range(width_cal):\n",
    "                for i_h in range(height_cal):\n",
    "                    output[i_d][i_w][i_h] += torch.sum(X[i_d:i_d + Y.shape[0], \\\n",
    "                                                         i_w:i_w + Y.shape[1], \\\n",
    "                                                         i_h:i_h + Y.shape[2]] * Y)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size[0] and input.shape[2] >= self.kernel_size[1] and input.shape[3] >= self.kernel_size[2], 'input to less!'\n",
    "        calculated = torch.zeros(self.out_channels, input.shape[1] - self.kernel_size[0] + 1, \\\n",
    "                                                    input.shape[2] - self.kernel_size[1] + 1, \\\n",
    "                                                    input.shape[3] - self.kernel_size[2] + 1)\n",
    "        for i_out in range(self.out_channels):\n",
    "            for i_in in range(self.in_channels):\n",
    "                self.conv3d(input[i_in], self.parameters[i_out, i_in], calculated[i_out])\n",
    "        if self.use_bias:\n",
    "            calculated += self.bias.reshape((self.out_channels, 1, 1, 1))\n",
    "        return calculated\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "        if self.use_bias:\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "# layer init parameters\n",
    "IN_CHANNEL = 3\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = (2, 2, 1)\n",
    "# input tensor: in_channels * depth * width * height\n",
    "DEPTH = 4\n",
    "WIDTH = 3\n",
    "HEIGHT = 2\n",
    "t3 = torch.arange(IN_CHANNEL * DEPTH * WIDTH * HEIGHT, dtype=torch.float)\n",
    "t3.resize_(IN_CHANNEL, DEPTH, WIDTH, HEIGHT)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(544.1322, grad_fn=<SumBackward0>), tensor(544.1321))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3d = nn.Conv3d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "l3d_my = myConv3d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "\n",
    "# compare\n",
    "l3d_my.parameters = l3d._parameters['weight'].data\n",
    "l3d_my.bias = l3d._parameters['bias'].data\n",
    "l3d(t3).sum(), l3d_my(t3).sum()\n",
    "\n",
    "# backward\n",
    "# l3d_my(t3).sum().backward()\n",
    "# l3d_my.parameters.grad, l3d_my.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 44.802\n",
      "epoch 20, loss 35.997\n",
      "epoch 30, loss 29.750\n",
      "epoch 40, loss 24.871\n",
      "epoch 50, loss 20.893\n",
      "epoch 60, loss 17.591\n",
      "epoch 70, loss 14.829\n",
      "epoch 80, loss 12.509\n",
      "epoch 90, loss 10.557\n",
      "epoch 100, loss 8.912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 0.4097, -0.3818],\n",
       "           [-0.4107,  0.3828]]]]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3d convolution operate\n",
    "def corr3d(X: torch.Tensor, K: torch.Tensor):  \n",
    "    d, w, h = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - d + 1, X.shape[1] - w + 1, X.shape[2] - h + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            for k in range(Y.shape[2]):\n",
    "                Y[i, j, k] = (X[i:i + d, j:j + w, k:k + h] * K).sum()\n",
    "    return Y\n",
    "\n",
    "X = torch.ones((6, 8, 7))\n",
    "X[:, 2:6, 3:5] = 0\n",
    "K = torch.tensor([[[1.0, -1.0], [-1.0, 1.0]]]) # <---target\n",
    "Y = corr3d(X, K)\n",
    "# conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "conv3d = myConv3d(1, 1, kernel_size=K.shape, bias=False)\n",
    "\n",
    "X = X.reshape((1, 6, 8, 7))\n",
    "Y = Y.reshape((1, 6, 7, 6))\n",
    "lr = 7e-4  # learn rate\n",
    "\n",
    "for i in range(100):\n",
    "    Y_hat = conv3d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    l.sum().backward()\n",
    "    with torch.no_grad():\n",
    "        conv3d.parameters -= lr * conv3d.parameters.grad\n",
    "    conv3d.zero_grad()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')\n",
    "conv3d.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[  0.,   1.,   2.,   3.],\n",
       "           [  4.,   5.,   6.,   7.]],\n",
       "\n",
       "          [[  8.,   9.,  10.,  11.],\n",
       "           [ 12.,  13.,  14.,  15.]],\n",
       "\n",
       "          [[ 16.,  17.,  18.,  19.],\n",
       "           [ 20.,  21.,  22.,  23.]]],\n",
       "\n",
       "\n",
       "         [[[ 24.,  25.,  26.,  27.],\n",
       "           [ 28.,  29.,  30.,  31.]],\n",
       "\n",
       "          [[ 32.,  33.,  34.,  35.],\n",
       "           [ 36.,  37.,  38.,  39.]],\n",
       "\n",
       "          [[ 40.,  41.,  42.,  43.],\n",
       "           [ 44.,  45.,  46.,  47.]]],\n",
       "\n",
       "\n",
       "         [[[ 48.,  49.,  50.,  51.],\n",
       "           [ 52.,  53.,  54.,  55.]],\n",
       "\n",
       "          [[ 56.,  57.,  58.,  59.],\n",
       "           [ 60.,  61.,  62.,  63.]],\n",
       "\n",
       "          [[ 64.,  65.,  66.,  67.],\n",
       "           [ 68.,  69.,  70.,  71.]]],\n",
       "\n",
       "\n",
       "         [[[ 72.,  73.,  74.,  75.],\n",
       "           [ 76.,  77.,  78.,  79.]],\n",
       "\n",
       "          [[ 80.,  81.,  82.,  83.],\n",
       "           [ 84.,  85.,  86.,  87.]],\n",
       "\n",
       "          [[ 88.,  89.,  90.,  91.],\n",
       "           [ 92.,  93.,  94.,  95.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 96.,  97.,  98.,  99.],\n",
       "           [100., 101., 102., 103.]],\n",
       "\n",
       "          [[104., 105., 106., 107.],\n",
       "           [108., 109., 110., 111.]],\n",
       "\n",
       "          [[112., 113., 114., 115.],\n",
       "           [116., 117., 118., 119.]]],\n",
       "\n",
       "\n",
       "         [[[120., 121., 122., 123.],\n",
       "           [124., 125., 126., 127.]],\n",
       "\n",
       "          [[128., 129., 130., 131.],\n",
       "           [132., 133., 134., 135.]],\n",
       "\n",
       "          [[136., 137., 138., 139.],\n",
       "           [140., 141., 142., 143.]]],\n",
       "\n",
       "\n",
       "         [[[144., 145., 146., 147.],\n",
       "           [148., 149., 150., 151.]],\n",
       "\n",
       "          [[152., 153., 154., 155.],\n",
       "           [156., 157., 158., 159.]],\n",
       "\n",
       "          [[160., 161., 162., 163.],\n",
       "           [164., 165., 166., 167.]]],\n",
       "\n",
       "\n",
       "         [[[168., 169., 170., 171.],\n",
       "           [172., 173., 174., 175.]],\n",
       "\n",
       "          [[176., 177., 178., 179.],\n",
       "           [180., 181., 182., 183.]],\n",
       "\n",
       "          [[184., 185., 186., 187.],\n",
       "           [188., 189., 190., 191.]]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myConv4d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple[int, int, int, int], stride: int = 1, bias: bool = True) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((self.out_channels, self.in_channels, *kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        if bias:\n",
    "            self.bias = torch.randn((self.out_channels, ), requires_grad=True)\n",
    "    \n",
    "    # calculate conv3d use kernel Y on X\n",
    "    @staticmethod\n",
    "    def conv4d(X: torch.Tensor, Y: torch.Tensor, output: torch.Tensor):\n",
    "        dim1_cal = X.shape[0] - Y.shape[0] + 1\n",
    "        dim2_cal = X.shape[1] - Y.shape[1] + 1\n",
    "        dim3_cal = X.shape[2] - Y.shape[2] + 1\n",
    "        dim4_cal = X.shape[3] - Y.shape[3] + 1\n",
    "        for i_1 in range(dim1_cal):\n",
    "            for i_2 in range(dim2_cal):\n",
    "                for i_3 in range(dim3_cal):\n",
    "                    for i_4 in range(dim4_cal):\n",
    "                        output[i_1][i_2][i_3][i_4] += torch.sum(X[i_1:i_1 + Y.shape[0], \\\n",
    "                                                            i_2:i_2 + Y.shape[1], \\\n",
    "                                                            i_3:i_3 + Y.shape[2], \\\n",
    "                                                            i_4:i_4 + Y.shape[3]] * Y)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size[0] and \\\n",
    "               input.shape[2] >= self.kernel_size[1] and \\\n",
    "               input.shape[3] >= self.kernel_size[2] and \\\n",
    "               input.shape[4] >= self.kernel_size[3], 'input to less!'\n",
    "        calculated = torch.zeros(self.out_channels, input.shape[1] - self.kernel_size[0] + 1, \\\n",
    "                                                    input.shape[2] - self.kernel_size[1] + 1, \\\n",
    "                                                    input.shape[3] - self.kernel_size[2] + 1, \\\n",
    "                                                    input.shape[4] - self.kernel_size[3] + 1)\n",
    "        for i_out in range(self.out_channels):\n",
    "            for i_in in range(self.in_channels):\n",
    "                self.conv4d(input[i_in], self.parameters[i_out, i_in], calculated[i_out])\n",
    "        if self.use_bias:\n",
    "            calculated += self.bias.reshape((self.out_channels, 1, 1, 1, 1))\n",
    "        return calculated\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "        if self.use_bias:\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "# layer init parameters\n",
    "IN_CHANNEL = 2\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = (2, 2, 1, 3)\n",
    "# input tensor: in_channels * depth * width * height\n",
    "DIM1 = 4\n",
    "DIM2 = 3\n",
    "DIM3 = 2\n",
    "DIM4 = 4\n",
    "t4 = torch.arange(IN_CHANNEL * DIM1 * DIM2 * DIM3 * DIM4, dtype=torch.float)\n",
    "t4.resize_(IN_CHANNEL, DIM1, DIM2, DIM3, DIM4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21986.5996, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4d_my = myConv4d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "\n",
    "# backward\n",
    "l4d_my(t4).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a47e9c955df6178b2e3d07b55dfed71eee1bcf21dc3da9caa7365344dfc4fda2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
