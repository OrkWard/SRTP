{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myConv1d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((out_channels, in_channels, kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        self.bias = torch.randn((out_channels, ), requires_grad=True)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size, 'input to less!'\n",
    "        calculated = torch.zeros((self.out_channels, input.size(1) - self.kernel_size + 1))\n",
    "        for i_out in range(self.out_channels):\n",
    "            for i_in in range(self.in_channels):\n",
    "                for i_w in range(calculated.shape[1]):\n",
    "                    # print(i_out, i_in, i_w)\n",
    "                    calculated[i_out][i_w] += torch.dot(self.parameters[i_out, i_in], input[i_in, i_w:i_w + self.kernel_size])\n",
    "        calculated += self.bias.reshape((self.out_channels, 1))\n",
    "        return calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN_CHANNEL = 1\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = 3\n",
    "HEIGHT = 3\n",
    "WIDTH = 3\n",
    "LENGTH = 4\n",
    "t = torch.arange(IN_CHANNEL * LENGTH, dtype=torch.float)\n",
    "t.resize_(IN_CHANNEL, LENGTH)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 3., 5.]],\n",
       " \n",
       "         [[1., 3., 5.]]]),\n",
       " tensor([2., 2.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = nn.Conv1d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE, dtype=torch.float)\n",
    "d2 = myConv1d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "# d2.parameters = d1._parameters['weight'].data\n",
    "# d2.bias = d1._parameters['bias'].data\n",
    "# d2(t), d1(t)\n",
    "d2(t).sum().backward()\n",
    "d2.parameters.grad, d2.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myConv2d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple[int, int], stride: int = 1) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((self.out_channels, self.in_channels, *kernel_size), requires_grad=True)\n",
    "\n",
    "        # init bias\n",
    "        self.bias = torch.randn((self.out_channels, ), requires_grad=True)\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.shape[0] == self.in_channels, 'in channels not match!'\n",
    "        assert input.shape[1] >= self.kernel_size[0] or input.shape[2] >= self.kernel_size[1], 'input to less!'\n",
    "        calculated = torch.zeros(self.out_channels, input.shape[1] - self.kernel_size[0] + 1, input.shape[2] - self.kernel_size[1] + 1)\n",
    "        for i_out in range(calculated.shape[0]):\n",
    "            for i_in in range(self.in_channels):\n",
    "                for i_w in range(calculated.shape[1]):\n",
    "                    for i_h in range(calculated.shape[2]):\n",
    "                        calculated[i_out][i_h][i_w] += torch.sum(self.parameters[i_out, i_in] * input[i_in, i_w:i_w + self.kernel_size[0], i_h:i_h + self.kernel_size[1]])\n",
    "        calculated += self.bias.reshape((self.out_channels, 1, 1))\n",
    "        return calculated\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "        self.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27.],\n",
       "         [28., 29., 30., 31.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN_CHANNEL = 2\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = (3, 3)\n",
    "HEIGHT = 4\n",
    "WIDTH = 4\n",
    "t1 = torch.arange(IN_CHANNEL * WIDTH * HEIGHT, dtype=torch.float)\n",
    "t1.resize_(IN_CHANNEL, WIDTH, HEIGHT)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 10.,  14.,  18.],\n",
       "           [ 26.,  30.,  34.],\n",
       "           [ 42.,  46.,  50.]],\n",
       " \n",
       "          [[ 74.,  78.,  82.],\n",
       "           [ 90.,  94.,  98.],\n",
       "           [106., 110., 114.]]],\n",
       " \n",
       " \n",
       "         [[[ 10.,  14.,  18.],\n",
       "           [ 26.,  30.,  34.],\n",
       "           [ 42.,  46.,  50.]],\n",
       " \n",
       "          [[ 74.,  78.,  82.],\n",
       "           [ 90.,  94.,  98.],\n",
       "           [106., 110., 114.]]]]),\n",
       " tensor([2., 2.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3 = nn.Conv2d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "d4 = myConv2d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "# d4.parameters = d3._parameters['weight'].data\n",
    "# d4.bias = d3._parameters['bias'].data\n",
    "# d3(t1), d4(t1)\n",
    "d4(t1).sum().backward()\n",
    "d4.parameters.grad, d2.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a47e9c955df6178b2e3d07b55dfed71eee1bcf21dc3da9caa7365344dfc4fda2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
