{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class myConv1d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((out_channels, in_channels, kernel_size))\n",
    "\n",
    "        # init bias\n",
    "        self.bias = torch.randn((out_channels))\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        calculated = torch.zeros((self.out_channels, input.size(1) - self.kernel_size + 1))\n",
    "        for i_out in range(self.out_channels):\n",
    "            for i_in in range(self.in_channels):\n",
    "                for i_w in range(calculated.shape[1]):\n",
    "                    # print(i_out, i_in, i_w)\n",
    "                    calculated[i_out][i_w] += torch.dot(self.parameters[i_out, i_in], input[i_in, i_w:i_w + self.kernel_size])\n",
    "        calculated += self.bias.reshape((self.out_channels, 1))\n",
    "        return calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN_CHANNEL = 1\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = 3\n",
    "HEIGHT = 3\n",
    "WIDTH = 3\n",
    "LENGTH = 4\n",
    "t = torch.arange(IN_CHANNEL * LENGTH, dtype=torch.float)\n",
    "t.resize_(IN_CHANNEL, LENGTH)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5475, 1.1547],\n",
       "         [0.9087, 1.6138]]),\n",
       " tensor([[0.5475, 1.1547],\n",
       "         [0.9087, 1.6138]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = nn.Conv1d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE, dtype=torch.float)\n",
    "d2 = myConv1d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "d2.parameters = d1._parameters['weight'].data\n",
    "d2.bias = d1._parameters['bias'].data\n",
    "d2(t), d1(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class myConv2d():\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple[int, int], stride: int = 1) -> None:\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.randn((self.out_channels, self.in_channels, *kernel_size))\n",
    "\n",
    "        # init bias\n",
    "        self.bias = torch.randn((self.out_channels))\n",
    "\n",
    "    def __call__(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        calculated = torch.zeros(self.out_channels, input.shape[1] - self.kernel_size[0] + 1, input.shape[2] - self.kernel_size[1] + 1)\n",
    "        for i_out in range(calculated.shape[0]):\n",
    "            for i_in in range(self.in_channels):\n",
    "                for i_w in range(calculated.shape[1]):\n",
    "                    for i_h in range(calculated.shape[2]):\n",
    "                        calculated[i_out][i_h][i_w] += torch.sum(self.parameters[i_out, i_in] * input[i_in, i_w:i_w + self.kernel_size[0], i_h:i_h + self.kernel_size[1]])\n",
    "        calculated += self.bias.reshape((self.out_channels, 1, 1))\n",
    "        return calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27.],\n",
       "         [28., 29., 30., 31.]]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN_CHANNEL = 2\n",
    "OUT_CHANNEL = 2\n",
    "KERNEL_SIZE = (3, 3)\n",
    "HEIGHT = 4\n",
    "WIDTH = 4\n",
    "t1 = torch.arange(IN_CHANNEL * WIDTH * HEIGHT, dtype=torch.float)\n",
    "t1.resize_(IN_CHANNEL, WIDTH, HEIGHT)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[13.9311, 14.7224],\n",
       "          [17.0962, 17.8874]],\n",
       " \n",
       "         [[ 2.0578,  1.8111],\n",
       "          [ 1.0712,  0.8245]]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[[13.9311, 17.0962],\n",
       "          [14.7224, 17.8874]],\n",
       " \n",
       "         [[ 2.0578,  1.0712],\n",
       "          [ 1.8111,  0.8245]]]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3 = nn.Conv2d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "d4 = myConv2d(IN_CHANNEL, OUT_CHANNEL, KERNEL_SIZE)\n",
    "d4.parameters = d3._parameters['weight'].data\n",
    "d4.bias = d3._parameters['bias'].data\n",
    "d3(t1), d4(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(8).reshape((1, 8))\n",
    "X.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a47e9c955df6178b2e3d07b55dfed71eee1bcf21dc3da9caa7365344dfc4fda2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
