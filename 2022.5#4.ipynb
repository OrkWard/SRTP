{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022.5#4\n",
    "为了更容易学习，本周我们从经典的线性神经网络开始完成整个神经网络的训练过程，尽可能理解过程中每一步发生了什么，包括定义简单的神经网络架构、数据处理、指定损失函数和训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import random\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据集\n",
    "#\n",
    "# 写一个带有噪声的模型来生成一个数据集。目标是通过有限样本的数据集\n",
    "# 恢复这个模型的参数。\n",
    "#\n",
    "# 下面是一个实现：生成一个包含500个样本的数据集，每个样本包括从标准\n",
    "# 正态分布中取出的两个参数。\n",
    "def sample_generator(w, b, sample_num):\n",
    "    # y = Xw + b + \\epsilon\n",
    "    X = torch.normal(0, 1, (sample_num, len(w)))\n",
    "    y:Tensor = X @ w + b\n",
    "    # std deviation = -.01\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "model_w = torch.tensor([2.022, 5.22])\n",
    "model_b = torch.tensor(4.2)\n",
    "features, labels = sample_generator(model_w, model_b, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "# 写一个生成器，每次抽取一小批样本来更新参数\n",
    "# 具体实现是将下标列表随机打乱，每次步进batch_size\n",
    "# 取出对应下标的样本\n",
    "'''\n",
    "在一些实现中我看到所有矩阵甚至列表运算都先将变量转为\n",
    "Tensor的情况，虽然感觉现阶段没必要仔细了解，但很好奇\n",
    "为什么Tensor运算的速度远比for循环快，以及为什么GPU\n",
    "的并行处理要比CPU快，GPU和CPU的并行处理硬件基础有很大\n",
    "不同吗？\n",
    "\n",
    "'''\n",
    "# 这个生成器是为了理解选择过程实现的，它必须先将所有数据\n",
    "# 读入内存，计算大量随机数，以及不断进行随机访问。这都会\n",
    "# 严重影响效率。\n",
    "def choose_batch(batch_size, features, labels):\n",
    "    sample_num = len(features)\n",
    "    # 生成一个目录列表并打乱\n",
    "    indices = list(range(sample_num))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, sample_num, batch_size):\n",
    "        # 注意batch_size不一定整除sample_num\n",
    "        batch_indeces = indices[i:min(i + batch_size, sample_num)]\n",
    "        yield features[batch_indeces], labels[batch_indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型参数\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 定义模型\n",
    "def linear(X, w, b):\n",
    "    return X @ w + b\n",
    "\n",
    "# 定义损失函数：均方损失\n",
    "def squared_loss(real_y:Tensor, y:Tensor):\n",
    "    return (real_y - y.reshape(real_y.shape)) ** 2 / 2\n",
    "\n",
    "# 定义优化算法：随机梯度下降\n",
    "def grediant_descent(params: list[Tensor], lr: float, batch_size: int):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            # 求导，乘以学习率\n",
    "            param -= param.grad * lr / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程\n",
    "# 超参数\n",
    "lr = 0.03\n",
    "batch_size = 10\n",
    "# 训练三个周期\n",
    "epoch_num = 3\n",
    "net = linear\n",
    "loss = squared_loss\n",
    "opt = grediant_descent\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for X, y in choose_batch(batch_size, features, labels):\n",
    "        l = loss(y, net(X, w, b))\n",
    "        l.sum().backward()\n",
    "        opt([w, b], lr, batch_size)     \n",
    "    # 一个周期结束，计算平均误差\n",
    "    train_l = loss(labels, net(features, w, b))\n",
    "    print(f'第{epoch + 1}个周期，损失{float(train_l.mean()):f}')\n",
    "print(f'w的估计误差: {model_w - w.reshape(model_w.shape)}')\n",
    "print(f'b的估计误差: {model_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自此完成了一个完整神经网络的训练。其中还有一点模糊之处，就是Pytorch的求导到底是如何进行的，backward函数究竟做了什么事。\n",
    "\n",
    "另外还有在优化算法中不使用torch.no_grad()修饰会报错，暂时没搞懂这个错误为何产生\n",
    "\n",
    "这个网络勉强能认为是一层，还远远不到”深度“的要求，下周期望搭建起一个多层神经网络"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a47e9c955df6178b2e3d07b55dfed71eee1bcf21dc3da9caa7365344dfc4fda2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hyperspectral')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
